{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs Project\n",
    "## Data Wrangling Internal Report\n",
    "\n",
    "Author: Rahma Ali\n",
    "\n",
    "12-Aug-2020\n",
    "\n",
    "This report summarizes data wrangling performed on WeRateDogs twitter account data. It is produced as part of my second project submission in Udacity professional track in data analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This project explores data from tweet archive of Twitter user [@dog_rates](https://twitter.com/dog_rates), also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs and provides a funny comment about the dog. These ratings consist of a numerator, which is almost always greater than 10 in all the ratings, and a denominator, that is mostly 10. This unusual way of rating is biased towards dog, who are indeed man's best friend and also make lovely pets! WeRateDogs currently has over 8 million followers and has received international media attention.\n",
    "\n",
    "## Sources of data\n",
    "Three datasets for this project from three different resources have been used:\n",
    "\n",
    "1- `twitter-archive-enhanced.csv`: contains basic tweet data for all 5000+ of WeRateDogs twitter account tweets offered under the course resources.\n",
    "\n",
    "2- `image_predictions.tsv`: contains WeRateDogs account tweet images predictions of breed of dog (or other object, animal, etc.) according to a neural network. The file is hosted on Udacity's server\n",
    "\n",
    "3- `tweet-json.txt` : contains WeRateDogs account tweets retweet and favorite count data from twitter_api.py\n",
    "\n",
    "## Data wrangling\n",
    "This section goes through the 3 steps of data wrangling that were performed on the data.\n",
    "\n",
    "### 1- Gather\n",
    "In this step, the datasets are gathered from 3 sources and imported. I will go over how I gathered each dataset in narrative first followed by code.\n",
    "\n",
    "1- `twitter-archive-enhanced.csv` was directly downloaded from course resources and read into python using pandas `read_csv` function.\n",
    "\n",
    "2- `image_predictions.tsv` was downloaded programmatically from a [url](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv) to Udacity server.\n",
    "\n",
    "3- `tweet-json.txt` was downloaded directly from course resources as well and read programmatically. I was not able to get confirmation on my twitter developer account by the time of submission to be able to get it from twitter API myself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Assess\n",
    "After importing all the needed dataframes, I started exploring all three both visually and programmatically. The following issues emerged from the data, which I categorized to quality and tidiness issues:\n",
    "\n",
    "#### 2.1 Quality issues\n",
    "- Missing dog classification observations are stored as `None`\n",
    "- Dog classification is not mutually exclusive\n",
    "- `tweet_id` variable is stored as integer instead of string\n",
    "- `source` variable values of urls is surrounded by tags\n",
    "- `timestamp` variable is stored as  string instead of datetime\n",
    "- `name` variable contains a number of wrong entries (`all`, `some`, ... etc)\n",
    "- Data contains retweets and replies and not just original tweets\n",
    "- Data contains tweets with no images\n",
    "- `name` variable contains the value `none` to represent missing values\n",
    "- The 4 dummy dog classification variables (`doggo`, `floofer`, `pupper`, `puppo`) are not mutually exclusive\n",
    "- Fix `rating_numerator` column values\n",
    "\n",
    "#### 2.2 Tidiness issues:\n",
    "- Dog classification values are present in column headers \n",
    "- Image prediction algorithm values are present in the column headers \n",
    "- Tweets observations are stored in multiple tables\n",
    "\n",
    "### 3- Clean\n",
    "After having assessed all the data issues, I started acting on them. The following list describes all the actions taken in order to fix all quality and data issues of the data:\n",
    "- Store values of columns headers in `image_predictions_df` in variables \n",
    "- Replace `None` values with `NaN` or empty strings in dog classification variables\n",
    "- Combine the 4 dummy dog classification variables (doggo, floofer, pupper, puppo) to one `dog_class` variable and fix rows with multiple dog classification\n",
    "- Convert `tweet_id` variable type to string in all dataframes to prepare for data merge\n",
    "- Remove extra tags in `source` variable\n",
    "- Fix non-capitalized and erroneous entries in `name` variable\n",
    "- Merge all three datasets on `tweet_id`and make a clean copy of the dataframe\n",
    "- Identify and remove retweets from the master dataframe and tweets without photos\n",
    "- Convert `timestamp` variable type to `datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data storing and analysis\n",
    "I saved the resulting master dataframe from the data cleaning phase as `twitter_archive_master.csv`, which I have attached to my project submission. I started exploring the data. You will find my insights in the `wrangle_act.html` report, also attached to my project submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
